{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import essential packages\n",
    "import torch \n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equation (7) in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anchorBboxSize(ah_i, aw_i, level):\n",
    "    minimum_size = 20\n",
    "    AH, AW = minimum_size * np.pow(2, level-1)\n",
    "    b_i = (np.log(ah_i/AH), np.log(aw_i/AW))\n",
    "\n",
    "    return b_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anchorBboxGenerator(b_i, level=1):\n",
    "    hidden_dim = 5\n",
    "    theta_dim = 10\n",
    "    theta_standard = torch.randn(theta_dim)\n",
    "\n",
    "    # two layers\n",
    "    residual_theta = F.linear(F.relu(F.linear(b_i, (2, hidden_dim))), (hidden_dim, theta_dim))\n",
    "    theta_b_i = theta_standard + residual_theta\n",
    "\n",
    "    return theta_b_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original RetinaNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OriginalRetinaNet(nn.Module):\n",
    "    num_anchors = 9 # 3 scales x 3 ratios\n",
    "\n",
    "    def __init__(self, num_classes=20):\n",
    "        super(OriginalRetinaNet, self).__init__()\n",
    "        self.fpn # = FPN50()\n",
    "        self.num_classes = num_classes\n",
    "        self.reg_head = self._make_head(self.num_anchors * 4) # 9 * 4 = 36\n",
    "        self.cls_head = self._make_head(self.num_anchors * self.num_classes) # 9 * 20 = 180\n",
    "\n",
    "    def forward(self, x):\n",
    "        fms = self.fpn(x)\n",
    "\n",
    "        loc_preds = []\n",
    "        cls_preds = []\n",
    "        for fm in fms:\n",
    "            loc_pred = self.loc_pred(fm)\n",
    "            cls_pred = self.cls_pred(fm)\n",
    "            loc_pred = loc_pred.permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, 4) # [N, 9*4, H, W] -> [N, H, W, 9*4] -> [N, H*W*9, 4]\n",
    "            cls_pred = cls_pred.permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, self.num_classes)\n",
    "\n",
    "            loc_preds.append(loc_pred)\n",
    "            cls_preds.append(cls_pred)\n",
    "\n",
    "        return torch.cat(loc_preds, 1), torch.cat(cls_preds, 1)\n",
    "\n",
    "    def _make_head(self, out_planes):\n",
    "        layers = []\n",
    "        for _ in range(4):\n",
    "            layers.append(nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1))\n",
    "            layers.append(nn.ReLU(True))\n",
    "        \n",
    "        layers.append(nn.Conv2d(256, out_planes, kernel_size=3, stride=1, padding=1))\n",
    "\n",
    "        return nn.Sequential(*layers) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MetaAnchor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaRetinaNet(nn.Module):\n",
    "    def __init__(self, num_classes=20):\n",
    "        super(MetaRetinaNet, self).__init__()\n",
    "        self.fpn # = FPN50()\n",
    "        self.num_classes = num_classes\n",
    "        self.reg_head = self._make_head(4) # 4\n",
    "        self.cls_head = self._make_head(self.num_classes) #20\n",
    "\n",
    "    def forward(self, x):\n",
    "        fms = self.fpn(x)\n",
    "\n",
    "        loc_preds = []\n",
    "        cls_preds = []\n",
    "        for fm in fms:\n",
    "            loc_pred = self.loc_pred(fm)\n",
    "            cls_pred = self.cls_pred(fm)\n",
    "            loc_pred = loc_pred.permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, 4) # [N, 4, H, W] -> [N, H, W, 94] -> [N, H*W, 4]\n",
    "            cls_pred = cls_pred.permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, self.num_classes)\n",
    "\n",
    "            loc_preds.append(loc_pred)\n",
    "            cls_preds.append(cls_pred)\n",
    "\n",
    "        return torch.cat(loc_preds, 1), torch.cat(cls_preds, 1)\n",
    "\n",
    "    def _make_head(self, out_planes):\n",
    "        layers = []\n",
    "        for _ in range(4):\n",
    "            layers.append(nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1))\n",
    "            layers.append(nn.ReLU(True))\n",
    "        \n",
    "        layers.append(nn.Conv2d(256, out_planes, kernel_size=3, stride=1, padding=1))\n",
    "\n",
    "        return nn.Sequential(*layers) \n",
    "\n",
    "def focal_loss_meta(b_i, cls_pred, cls_label, loc_pred, loc_label):\n",
    "    '''\n",
    "    bi = [N,2]\n",
    "    cls_pred = [N,20]\n",
    "    cls_label = [N,]\n",
    "    reg_pred = [N,4]\n",
    "    reg_label = [N,4]\n",
    "    \n",
    "    '''\n",
    "\n",
    "    alpha = 0.25\n",
    "    gamma = 2\n",
    "    num_classes = 20\n",
    "\n",
    "    t = torch.eye(num_classes + 1) (loc_label, )\n",
    "    t = t[:, 1:] # t is one-hot vector\n",
    "\n",
    "    p = F.logsigmoid(cls_pred)\n",
    "    pt = p*t + (1-p)*(1-t) # pt = p if t > 0 else 1 - p\n",
    "\n",
    "    m = alpha*t + (1-alpha)*(1-t)\n",
    "    m = m * (1-pt).pow(gamma)\n",
    "\n",
    "    weight = anchorBboxGenerator(b_i, )\n",
    "\n",
    "    cls_loss = F.binary_cross_entropy_with_logits(x, t, m, size_average=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bitpy3venv2d141099d5944d30a30973a7685a490a",
   "display_name": "Python 3.6.9 64-bit ('py3': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
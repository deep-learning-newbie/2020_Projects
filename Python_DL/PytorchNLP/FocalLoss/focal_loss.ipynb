{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2., alpha=0.25, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "\n",
    "        self.gamma = gamma \n",
    "        self.alpha = alpha \n",
    "        self.size_average = size_average\n",
    "\n",
    "        if isinstance(alpha, (float, int)):\n",
    "            self.alpha = torch.Tensor([alpha, 1. - alpha])\n",
    "        elif isinstance(alpha, list):\n",
    "            self.alpha = torch.Tensor(alpha)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim() > 2:\n",
    "            input = input.view(input.size(0), input.size(1), -1) # N, C, H, W => N, C, H * W\n",
    "            input = input.transpose(1, 2)\n",
    "            input = input.contiguous().view(-1, input.size(2)) # N, C, H * W -> N * H * W, C\n",
    "            \n",
    "        target = target.view(-1, 1)\n",
    "        \n",
    "        logpt = F.log_softmax(input, dim=1)\n",
    "        logpt = logpt.gather(1, target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = logpt.exp()\n",
    "        \n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type() != input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            \n",
    "            at = self.alpha.gather(0, target.data.view(-1))\n",
    "            logpt = logpt * at\n",
    "            \n",
    "        loss = -1 * pow(1-pt, self.gamma) * logpt\n",
    "        \n",
    "        if self.size_average:\n",
    "            return loss.mean()\n",
    "        \n",
    "        return loss.sum()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focal Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F \n",
    "\n",
    "import os, sys, random, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (256000) must match the size of tensor b (128000) at non-singleton dimension 0",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-7ed4c7164ef9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#target = target.cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0moutput0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFocalLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0moutput1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0moutput2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-05e97e59751d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mlogpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogpt\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlogpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (256000) must match the size of tensor b (128000) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "max_err = 0\n",
    "\n",
    "x = torch.rand(128000, 2) * random.randint(1, 10)\n",
    "#x = x.cuda()\n",
    "\n",
    "target = torch.rand(128000).ge(0.1).long()\n",
    "#target = target.cuda()\n",
    "\n",
    "output0 = FocalLoss()(x, target)\n",
    "output1 = nn.CrossEntropyLoss()(x, target)\n",
    "output2 = nn.BCEWithLogitsLoss()(x, torch.rand(x.shape).random_(2))\n",
    "\n",
    "\n",
    "print(\"Focal Loss: {}\".format(output0))\n",
    "print(\"CE Loss: {}\".format(output1))\n",
    "print(\"BCE Loss: {}\".format(output2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RetinaNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, features_in, features_out=256, num_anchors=9, num_classes=80):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "\n",
    "        self.num_anchors = num_anchors\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.conv1 = nn.Conv2d(features_in, features_out, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(features_out, features_out, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(features_out, features_out, kernel_size=3, padding=1)\n",
    "        self.act3 = nn.ReLU()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(features_out, features_out, kernel_size=3, padding=1)\n",
    "        self.act4 = nn.ReLU()\n",
    "\n",
    "        self.output = nn.Conv2d(features_out, num_anchors * num_classes, kernel_size=3, padding=1)\n",
    "        self.output_act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        print(\"layer 1: shape {}\".format(out.shape))\n",
    "        out = self.act1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        print(\"layer 2: shape {}\".format(out.shape))\n",
    "        out = self.act2(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        print(\"layer 3: shape {}\".format(out.shape))\n",
    "        out = self.act3(out)\n",
    "\n",
    "        out = self.conv4(out)\n",
    "        print(\"layer 4: shape {}\".format(out.shape))\n",
    "        out = self.act4(out)\n",
    "\n",
    "        out = self.output(out)\n",
    "        print(\"layer output: shape {}\".format(out.shape))\n",
    "        out = self.output_act(out)\n",
    "\n",
    "        # out is B x C x W x H, with C = n_classes * n_anchors\n",
    "        print(out.shape)\n",
    "        out1 = out.permute(0, 2, 3, 1) # B, C, W, H => B, W, H, C\n",
    "\n",
    "        batch_size, width, height, channels = out1.shape\n",
    "\n",
    "        out2 = out1.view(batch_size, width, height, self.num_anchors, self.num_classes) # B, W, H, C => B, W, H, #anchors, #classes\n",
    "\n",
    "        return out2.contiguous().view(x.shape[0], -1, self.num_classes) # B, W, H, #anchors, #classes => B, W * H * #anchors, #classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 256, 256, 3) * 255\n",
    "\n",
    "x = torch.clamp(x, 0, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "layer 1: shape torch.Size([1, 256, 256, 3])\nlayer 2: shape torch.Size([1, 256, 256, 3])\nlayer 3: shape torch.Size([1, 256, 256, 3])\nlayer 4: shape torch.Size([1, 256, 256, 3])\nlayer output: shape torch.Size([1, 720, 256, 3])\ntorch.Size([1, 720, 256, 3])\n"
    }
   ],
   "source": [
    "classification = ClassificationModel(features_in=256)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[0.6675, 0.8112, 0.5319,  ..., 0.4915, 0.5656, 0.4535],\n         [0.4240, 0.4811, 0.6332,  ..., 0.4638, 0.4593, 0.3847],\n         [0.4679, 0.5647, 0.7307,  ..., 0.4766, 0.5034, 0.5730],\n         ...,\n         [0.4156, 0.6728, 0.4103,  ..., 0.7202, 0.5673, 0.5896],\n         [0.4846, 0.7695, 0.6661,  ..., 0.1779, 0.2749, 0.2846],\n         [0.6998, 0.4297, 0.5478,  ..., 0.3712, 0.7847, 0.5753]]],\n       grad_fn=<ViewBackward>)"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[0., 0., 1.,  ..., 1., 0., 1.],\n         [0., 0., 1.,  ..., 1., 1., 0.],\n         [0., 1., 1.,  ..., 1., 0., 0.],\n         ...,\n         [0., 1., 1.,  ..., 1., 0., 0.],\n         [1., 0., 1.,  ..., 1., 1., 1.],\n         [1., 0., 1.,  ..., 1., 0., 1.]]])"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "alpha = 0.25\n",
    "gamma = 2.0\n",
    "\n",
    "targets = torch.zeros(classification.shape)\n",
    "targets = torch.where(torch.lt(classification, 0.4), targets, 1. - targets)\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[0.7500, 0.7500, 0.2500,  ..., 0.2500, 0.7500, 0.2500],\n         [0.7500, 0.7500, 0.2500,  ..., 0.2500, 0.2500, 0.7500],\n         [0.7500, 0.2500, 0.2500,  ..., 0.2500, 0.7500, 0.7500],\n         ...,\n         [0.7500, 0.2500, 0.2500,  ..., 0.2500, 0.7500, 0.7500],\n         [0.2500, 0.7500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n         [0.2500, 0.7500, 0.2500,  ..., 0.2500, 0.7500, 0.2500]]])"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "alpha_factor = torch.ones(targets.shape) * alpha\n",
    "alpha_factor = torch.where(torch.eq(targets, 1.), alpha_factor, 1 - alpha_factor)\n",
    "alpha_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[0.3734, 0.2478, 0.3728,  ..., 0.4098, 0.1884, 0.3931],\n         [0.1092, 0.3380, 0.2173,  ..., 0.3912, 0.5951, 0.3356],\n         [0.3776, 0.3895, 0.4681,  ..., 0.2609, 0.2131, 0.2418],\n         ...,\n         [0.2737, 0.3216, 0.4888,  ..., 0.5191, 0.3035, 0.1805],\n         [0.2249, 0.3027, 0.4130,  ..., 0.2277, 0.3505, 0.2988],\n         [0.3613, 0.2720, 0.5198,  ..., 0.3241, 0.2999, 0.4858]]],\n       grad_fn=<SWhereBackward>)"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "focal_weight = torch.where(torch.eq(targets, 1.), 1. - classification, classification)\n",
    "focal_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[0.1046, 0.0460, 0.0347,  ..., 0.0420, 0.0266, 0.0386],\n         [0.0089, 0.0857, 0.0118,  ..., 0.0383, 0.0885, 0.0845],\n         [0.1069, 0.0379, 0.0548,  ..., 0.0170, 0.0341, 0.0439],\n         ...,\n         [0.0562, 0.0259, 0.0597,  ..., 0.0674, 0.0691, 0.0244],\n         [0.0126, 0.0687, 0.0426,  ..., 0.0130, 0.0307, 0.0223],\n         [0.0326, 0.0555, 0.0675,  ..., 0.0263, 0.0674, 0.0590]]],\n       grad_fn=<MulBackward0>)"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "focal_weight = alpha_factor * torch.pow(focal_weight, gamma)\n",
    "focal_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[-0.4675, -0.2847,  0.4665,  ...,  0.5273, -0.2088,  0.4993],\n         [-0.1156, -0.4125,  0.2450,  ...,  0.4962,  0.9040, -0.4088],\n         [-0.4741,  0.4934,  0.6312,  ...,  0.3024, -0.2397, -0.2768],\n         ...,\n         [-0.3198,  0.3880,  0.6711,  ...,  0.7320, -0.3617, -0.1990],\n         [ 0.2548, -0.3606,  0.5328,  ...,  0.2584,  0.4316,  0.3550],\n         [ 0.4483, -0.3174,  0.7335,  ...,  0.3918, -0.3565,  0.6651]]],\n       grad_fn=<AddBackward0>)"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "bce = -(targets * torch.log(classification)) + (1. - targets)*torch.log(1. - classification)\n",
    "bce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[-0.0489, -0.0131,  0.0162,  ...,  0.0221, -0.0056,  0.0193],\n         [-0.0010, -0.0353,  0.0029,  ...,  0.0190,  0.0800, -0.0345],\n         [-0.0507,  0.0187,  0.0346,  ...,  0.0051, -0.0082, -0.0121],\n         ...,\n         [-0.0180,  0.0100,  0.0401,  ...,  0.0493, -0.0250, -0.0049],\n         [ 0.0032, -0.0248,  0.0227,  ...,  0.0033,  0.0133,  0.0079],\n         [ 0.0146, -0.0176,  0.0495,  ...,  0.0103, -0.0240,  0.0392]]],\n       grad_fn=<MulBackward0>)"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "clss_loss = focal_weight * bce\n",
    "clss_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another implementation\n",
    "def focal_loss(y_true, y_pred):\n",
    "    pt1 = torch.where(torch.eq(targets, 1.), )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('py3': venv)",
   "language": "python",
   "name": "python36964bitpy3venvb342451405614fcdbfdacecf10a975f5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}